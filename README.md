# Custom Language Model

自定义小语言模型，从预训练到指令微调全过程。

看了这么多大模型，但是没有自己实践一下，对于其中结构没有很好了解，于是打算自己使用 `pytorch` 实现一个 Toy 级别的 Language Model。不在意模型效果，旨在整个过程中能够加深理解，弥补理论和实践过程中的 gap。

# 进展

- 完成模型结构
- 完成 tokenizer 训练
- 完成模型预训练
- 完成指令微调训练

# 最后

仓库中模型结构参考 [transformers](https://github.com/huggingface/transformers/tree/main) 中 `llama` 和 `Qwen2` 的实现。

仓库所有内容均是我对大模型理解形成的，不一定正确，如果有错误欢迎提出。